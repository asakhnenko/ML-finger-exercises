{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux, MLDatasets, Statistics\n",
    "using Parameters: @with_kw\n",
    "using Printf\n",
    "\n",
    "@with_kw mutable struct Args\n",
    "    learning_rate::Float64 = 3e-4       \n",
    "    batch_size::Int = 1024   \n",
    "    epochs::Int = 10\n",
    "    verbose::Bool = true\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Hello World\" in Julia\n",
    "#### MLP on MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_data(args)\n",
    "    \"\"\" Loading data \"\"\"\n",
    "    # Loading Dataset\n",
    "    X_train, y_train = MLDatasets.MNIST.traindata(Float32)\n",
    "    X_test,  y_test  = MLDatasets.MNIST.testdata(Float32)\n",
    "\n",
    "    # Preprocessing steps\n",
    "    X_train, X_test = Flux.flatten(X_train), Flux.flatten(X_test)\n",
    "    y_train, y_test = Flux.onehotbatch(y_train, 0:9), Flux.onehotbatch(y_test, 0:9)\n",
    "\n",
    "    # Batching\n",
    "    train_data = Flux.Data.DataLoader(X_train, y_train, batchsize=args.batch_size, shuffle=true)\n",
    "    test_data = Flux.Data.DataLoader(X_test, y_test, batchsize=args.batch_size)\n",
    "    \n",
    "    if args.verbose\n",
    "        @printf \"------Loaded Dataset statistics------\\n\"\n",
    "        @printf \"Training data amount :   %d\\n\" size(X_train, 2)\n",
    "        @printf \"Training data size   :   %d\\n\" size(X_train, 1)\n",
    "        @printf \"-------------------------------------\\n\"\n",
    "        @printf \"Testing data amount  :   %d\\n\" size(X_test, 2)\n",
    "        @printf \"Testing data size    :   %d\\n\" size(X_test, 1)\n",
    "    end\n",
    "    \n",
    "    return train_data, test_data\n",
    "end\n",
    "\n",
    "function loss(model)\n",
    "    \"\"\" Loss function \"\"\"\n",
    "    function (x, y)\n",
    "        Flux.logitcrossentropy(model(x), y)\n",
    "    end\n",
    "end\n",
    "\n",
    "function loss_all(data, loss_func)\n",
    "    \"\"\" Calculate loss on the whole dataset \"\"\"\n",
    "    sum([loss_func(x, y) for (x, y) in data]) / length(data)\n",
    "end\n",
    "\n",
    "function accuracy(data, model)\n",
    "    \"\"\" Calculate accuracy on the whole dataset \"\"\"\n",
    "    accuracy = 0\n",
    "    for (x, y) in data\n",
    "        predicted  = Flux.onecold(cpu(model(x)))\n",
    "        true_label = Flux.onecold(cpu(y))\n",
    "        accuracy += sum(Flux.onecold(cpu(model(x))) .== Flux.onecold(cpu(y))) * 1 / size(x, 2)\n",
    "    end\n",
    "    accuracy / length(data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Loaded Dataset statistics------\n",
      "Training data amount :   60000\n",
      "Training data size   :   784\n",
      "-------------------------------------\n",
      "Testing data amount  :   10000\n",
      "Testing data size    :   784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 1\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 2\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 3\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 4\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 5\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 6\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 7\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 8\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 9\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n",
      "┌ Info: Epoch 10\n",
      "└ @ Main C:\\Users\\alona\\.julia\\packages\\Flux\\Fj3bt\\src\\optimise\\train.jl:121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Model Performance test--------\n",
      "Accuracy on training data: 92.248 \n",
      "Accuracy on testing data : 92.366 \n",
      "-------------------------------------\n",
      "Loss on training data: 0.280 \n",
      "Loss on testing data : 0.274 \n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "\n",
    "# Loading Data\n",
    "train_data, test_data = load_data(args)\n",
    "\n",
    "# Constructing the model\n",
    "model = Chain(\n",
    "    Dense(28 * 28, 32, relu),\n",
    "    Dense(32, 10)\n",
    ")\n",
    "\n",
    "# Training\n",
    "Flux.@epochs args.epochs Flux.train!(\n",
    "    loss(model), \n",
    "    params(model), \n",
    "    train_data, \n",
    "    ADAM(args.learning_rate)\n",
    ")\n",
    "\n",
    "if args.verbose\n",
    "    @printf \"-------Model Performance test--------\\n\"\n",
    "    @printf \"Accuracy on training data: %0.3f \\n\" accuracy(train_data, model) * 100\n",
    "    @printf \"Accuracy on testing data : %0.3f \\n\" accuracy(test_data, model) * 100\n",
    "    @printf \"-------------------------------------\\n\"\n",
    "    @printf \"Loss on training data: %0.3f \\n\" loss_all(train_data, loss(model)) \n",
    "    @printf \"Loss on testing data : %0.3f \\n\" loss_all(test_data, loss(model)) \n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
